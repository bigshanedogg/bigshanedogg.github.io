---
layout: post
date: 2019-02-04 19:36
title: Multilingual BERT
description: Scrutinzing Multilingual BERT inside
comments: True
category: 
- deep_learning
- paper_review
tags:
- deep_learning
- paper_review
- language_model
- bert
---
2019 if Kakao 컨퍼런스 후기



(Kakao 컨퍼런스 입장)

2019 if Kakao는 29일에서 30일까지 이틀에 걸쳐 진행되었습니다. 대체로 29일에는 'FE', 30일에는 '데이터'에 대한 내용이 주를 이루었습니다. 



0. 키노트



(키노트 시작 1분 전)

행사는 약 40분 간의 키노트로 시작했고, 이어서 1 타임당 약 5개의 세션으로 원하는 세션을 골라들을 수 있었습니다. 사실 키노트에 대한 이야기도 흥미롭긴 했습니다. 

카카오에서 오픈소스로 진행 중인 DAPP 개발/배포 플랫폼 Kytln 이야기를 듣다보니 한빛출판사에서 주관했던 컨퍼런스인 Devground 세션이 떠올랐기 때문입니다. Devground에서 들은 DAPP(Decentralized Application) 게임 개발 세션에 'DAPP 플랫폼에서 유저가 클리어한 퀘스트, 얻은 아이템, 사냥 결과 등의 모든 데이터가 분산되어 저장된다.'는 내용이 있었습니다. 사실 당시엔 로그를 분산 기록하는 것의 가치를 공감하기 어려웠는데, 이번 키노트에서 '디지털 소작농'라는 단어를 듣고 생각이 조금 달라졌습니다. 당연한 이야기지만, 어플리케이션 내 혹은 게임 내에서의 사용자의 모든 활동은 기록되고, 서비스 제공자가 부가가치를 생산하는데 사용됩니다. 그런 의미에서 사용자가 생산한 데이터가 서비스 제공자의 사유재산이 된다는 의미이고, 이러한 데이터를 일구어 상납해 그 대가로 (데이터를 생산할 수 있는) 서비스를 제공받는 구조가 중세의 농노제와 같다는 이야기입니다. 요즘처럼 모바일 기기 없이 살기 어려운 때에는 더더욱 그 의존 관계를 바꾸기 어려워지구요.

구글에 노출되는 광고가 사용자 검색 이력에 따라 변한다는 것을 모르는 분들은 없으실 겁니다. 한 유투버가 '검색 행위가 아니라도 사용자 mic 입력에 따라 크롬 광고 노출이 변한다.'는 내용의 영상을 올린 적이 있었습니다. 구글이 '그렇지 않다.'고 대응했지만, 이런 내용의 기사를 볼 때마다 내 데이터를 이용해 어떤 일이 일어나도 알아차리기 어렵고, (내가 어느샌가 눌러놓은 '동의' 버튼 때문에) 막는 것은 더욱 더 어렵겠다는 생각이 듭니다. 평소 이런 우려를 종종하는 저로서는 DAPP 기반 플랫폼과 블록체인 기반의 'Decentralization(탈중앙화)'이 데이터에 적용될 때의 가치에 대해 다시 한번 생각해보는 계기가 되었습니다.



키노트가 끝나고 이제 드디어 세션을 들으러 돌아다니게 됩니다. 저는 관심분야인 '자연어처리'에 관련된 세션을 주로 들었는데, 각 세션별로 인상 깊었던 내용을 조금씩 정리해보겠습니다. 



(30일 세션 목록)



1. 금융사기 잡는 카카오뱅크의 데이터사이언스

사내 게시판에 DS 시험 관련 버즈가 많은 요즘이라 더욱 궁금했던 세션입니다. 데이터 사이언스와 엔지니어링의 경계는 무엇이고 카카오에서 DS는 어떻게 활용되고 있을지 궁금해서 들었는데요, 사실 데이터 사이언스와 데이터 엔지니어링 사이에 명확한 구분은 없이 EDA, 모델링, 서비스화까지 쭉 다 하는 듯했습니다.

카카오뱅크 주 사용자층 분석으로 시작해서 주 내용은 금융거래 Anomaly Detection 파이프라인에 대한 설명이 이어졌습니다. 사용자의 앱 활동 일부 라벨링하고, ANN(Approximate Nearest Neighbor)로 정상/사기로 데이터를 나누어 VAE를 학습시킨 뒤 정상 데이터의 X - X'와 사기 데이터의 X - X'를 1d CNN으로 피딩하는 파이프라인입니다. 결과적으로 성능이 압도적으로 향상되진 않았지만 유의미한 성능 향상이 있었고, 여러 핫한 모델들이 나오는 시점에서 모델보다 전처리와 feature selection의 중요성에 대해 다시 한 번 느낄 수 있었다고 합니다.

흥미로웠던 부분은 정상 데이터가 전체의 99.9%, 사기 데이터가 전체의 0.1%였기 때문에 발생한 data imbalance 문제였는데, 각각 언더샘플링과 오버샘플링하고 추가로 분류기 학습시 loss matrix를 적용했다고 합니다. 또, 일반적인 클러스터링으로 특징이 잘 드러나지 않는 경우, TDA(Topological Data Analysis)를 이용하면 특징을 잘 볼 수 있는 경우가 있다는 부분도 흥미로웠습니다. 이렇게 키워드를 수집하다보면 언젠가 필요한 순간이 있을 거라고 생각하며 열심히 주워담았습니다.

VAE를 이용해 문장을 압축하고 클러스터링하는 태스크를 하는 중이라 생성 모델에 대해서 더 자세하게 여쭤보고 싶었는데, 앱 활동은 로그인/대출 조회 등의 활동을 정형화한 데이터라 자연어 생성 모델은 아직 테스트해보지 않았다고 하셔서 다음 세션을 노리기로 했습니다. 



2. 기계독해 및 웹검색에 기반한 한국어 오픈 도메인 질의 응답 시스템

개인적으로는 가장 재밌는 세션이었습니다. 다른 세션이 주로 데이터를 탐험하고 그것을 처리하면서 느낀 인사이트와 그 과정들에 대한 이야기가 주인데 반해, 이 세션은 처음부터 끝까지 완성된 결과물에 대한 구조 설명과 모듈별 기능에 대한 세션이었습니다. 마치 논문 한 편을 읽은 듯한 밀도 높은 경험이 쭉 밀려왔습니다. (한 슬라이드도 놓치고 싶지 않아 열심히 사진을 찍었는데, 나중에 웹으로 공개한다고...)

시작은 벤치마크 대비 추론 성능 비교부터 시작했습니다. <b> '모델 개발 후 추론 성능 향상을 위해 C++로 포팅, CPU/GPU 등 다양한 환경에서 테스트하며 최적화한 결과 평균 1.9sec 내로 최적화했다.' </b>는 걸 듣고 나중에 서비스 최적화를 위해선 C++ 포팅은 선택이 아니라 필수겠구나, 하는 생각이 들었습니다. 

단순히 주어진 질문에 적절한 답을 내는 1개의 태스크를 수행하는 모델이지만, 그 안에 굉장히 많은 모듈들로 이루어져있었습니다. 

- 1) 쿼리 특성 분석
  - 육하원칙 판별
  - 서술형/단답형 판별
  - 쿼리 키워드 추출
  - 동사-명사 변환
  - 검색 쿼리 단순화
  - 쿼리 특성 기반 쿼리 확장
- 2) 검색쿼리 생성
- 3) 웹 검색
  - 답으로 내놓을 정보를 얻어가는 과정으로, 이 부분은 다음 검색 엔진 결과를 사용했다고 합니다.
  - 검색 결과 chunk화 및 중복 제거
- 4) 검색 결과 전처리
- 5) 기계 독해
  - 기본적으로 enc-dec 구조로, decoder는 빠른 추론을 위해 C++로 구현
- 6) 최종 정답 추출
  - 단답형 정답 추출기
  - 서술형 정답 추출기
  - 여러가지 웹 검색 결과 각각에서 정답을 추출하고 앙상블

<b>그 안에 정말 많은 모듈들이 들어가 있고, CNN, transformer 등의 DL 모델 뿐 아니라 TF-IDF나 PMI를 포함한 기법, 한국어 문장에서 정보량 scoring하는 많은 규칙을 혼합한 구조였습니다.</b> 나중에 여쭤보니 이러한 규칙이나 scoring 기법은 예전 parser를 만들 때 사용한 방법을 조금씩 실험해가며 적용하셨다는데, 정말 어마어마했습니다. 이어서 이 모델을 고도화시키기 위한 향후 태스크들까지 어느 것 하나 흥미롭지 않은 슬라이드가 없었습니다. 모델 구조부터 각 부분의 레이아웃까지 슬라이드에 모두 포함하셨는데, 여기서 자부심과 실력을 엿볼 수 있어 더욱 더 인상 깊은 세션이었습니다. 

<b>카카오 질의응답 데모 사이트 링크</b>



3. Korean의 Korean 체험기

전체적으로 Jamo piece와 BERT에 대해 다룬 세션이었습니다. 평소 모델을 개발할 때 char이나 word 단위 임베딩을 자주 사용했는데, (word는 끝도 없고) 한글 character도 11000자 가까이 되고, 완성형도 2350자가 넘기 때문에 차원수 문제가 가장 골치거리였는데, Jamo piece라는 시도할 방향이 하나 더 생긴 것 같아 반가웠습니다. 개인적으로 분류기는 word2vec이나 fastText를 사용하면 어느 정도 해결이 되지만, decoder가 포함된 생성 모델은 word2vec류의 representation을 이용하면 생성이 그럴듯하게 되지 않았습니다. (대부분의 단어가 구의 표피에 머무는 것 때문이 아닐까 추측하고 있습니다.) 그래서 혹시 word2vec이나 fastText를 이용한 생성 모델 노하우(이 과정이 가능한지 아닌지, 맞다면 어떤 부분을 고려해야 하는지)에 대해서 배우고 싶었는데, 아쉽게도 앞의 세션들과  마찬가지로 들을 수 없어 아쉬웠습니다.

하지만 세션에서 언급한대로 '자음과 모음' 단위로 쪼갠다면 초중종성과 숫자 등등 모두 합해서 200차원 내외로 해결할 수 있을 것 같아 좋은 수확이었습니다. BERT는 XLNET 이전까지 enc-dec 구조의 SOTA로 유명한 모델이고, 더 좋은 설명들이 많아 줄였습니다.



마무리

가장 인상적인 점은 역시 기술에 대한 태도였습니다. 전체 비즈니스 기능 중 일부분을 담당하는 작은 모듈이기 때문에 큰 비즈니스적 가치를 발견하기 어려울 수 있음에도 지속적으로 연구해 전체 파이프라인의 성능을 최적화한 사례,  장애가 생겼을 때 단순한 양적 확장이 아니라 새로운 아키텍쳐를 고민해 최적의 scale-out을 이룬 사례. 이런 사례들은 기술에 대한 끈질김과 호기심을 바탕으로 한 지속적인 고민과 토론의 산물이었고, 그 과정에서 느낀 인사이트들이라 더 진하게 다가왔습니다. 특히, <b>'기계독해 및 웹검색에 기반한 한국어 오픈 도메인 질의 응답 시스템'</b> 세션은 시종일관 담담한 톤으로 말씀하셨지만, 질의응답이라는 하나의 서비스를 만들기 위해 그 안에 녹여낸 수많은 분류기와 모델, 한국어 처리 노하우, 그리고 약 1년에 걸친 기간 동안 그것들을 기능적으로, 퍼포먼스적으로 최적화하겠다는 집념과 열정을 엿볼 수 있어서 더욱 값진 세션이었습니다. 하나하나 모듈에 대해 했던 깊은 고민과 인사이트를 아낌없이 공유하는 모습까지... 너무 멋있고 감동적인 세션이었습니다.

2019 if Kakao 행사 시작 전 코엑스에 입장하는 수많은 사각형 백팩들을 보며 설렜던만큼, 카카오/카카오뱅크/카카오페이 등 많은 부서에서 이루어지는 실무적인 고민을 간접적으로 느끼고 인사이트를 공유 할 수 있는 시간이었습니다. 